{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEnRhro7+WLcgio/he9yiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujpunekar20/video-translator/blob/main/flask_vt_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install flask, ngrok and other libraries/modules\n"
      ],
      "metadata": {
        "id": "IRHr2RW7SDYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI0oGiTEM8wf"
      },
      "outputs": [],
      "source": [
        "# !pip install pyngrok\n",
        "# from pyngrok import ngrok\n",
        "# !pip install flask moviepy transformers TTS librosa gdown\n",
        "# !pip install git+https://github.com/openai/whisper.git\n",
        "# !pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "# !gdown --id 1_OvqStxNxLc7bXzlaVG5sz695p-FVfYY -O checkpoints/wav2lip_gan.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install flask pyngrok\n",
        "\n",
        "# Ensure Wav2Lip and its requirements are installed\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git\n",
        "!pip install -r Wav2Lip/requirements.txt\n",
        "!pip install librosa moviepy gdown\n",
        "!pip install transformers TTS\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "y-0UUIwrOdKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken cr_2iljjpUSwsZy9UlIuGW0aIiDutE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6arNrS9WgjG",
        "outputId": "23b45878-969d-4fc6-829c-183d684af31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flask app"
      ],
      "metadata": {
        "id": "-Trqj0bWWt7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from pyngrok import ngrok\n",
        "import whisper\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from TTS.api import TTS\n",
        "import moviepy.editor as mp\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/upload\", methods=[\"POST\"])\n",
        "def upload():\n",
        "    video_file = request.files['video']\n",
        "    ref_audio_file = request.files['ref_audio']\n",
        "\n",
        "    video_path = os.path.join(\"uploads\", video_file.filename)\n",
        "    ref_audio_path = os.path.join(\"uploads\", ref_audio_file.filename)\n",
        "\n",
        "    video_file.save(video_path)\n",
        "    ref_audio_file.save(ref_audio_path)\n",
        "\n",
        "    # Extract audio from video\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio_file = \"extracted_audio.wav\"\n",
        "    video.audio.write_audiofile(audio_file)\n",
        "\n",
        "    # Transcribe Audio with Whisper\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_file)\n",
        "    transcribed_text = result['text']\n",
        "    print(\"Transcribed Text:\", transcribed_text)\n",
        "\n",
        "    # Translate Text to Hindi using NLLB-200 model\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    translation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Define language codes\n",
        "    src_lang = \"eng_Latn\"\n",
        "    tgt_lang = \"hin_Deva\"\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"{src_lang} {transcribed_text} {tgt_lang}\"\n",
        "\n",
        "    # Tokenize and translate\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "    translated_ids = translation_model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Translated Text:\", translated_text)\n",
        "\n",
        "    # Convert Text to Speech in Hindi\n",
        "    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
        "    tts_audio_path = \"translated_audio.mp3\"\n",
        "    tts.tts_to_file(translated_text,\n",
        "                    file_path=tts_audio_path,\n",
        "                    speaker_wav=ref_audio_path, # Use uploaded reference audio file\n",
        "                    language=\"hi\")\n",
        "\n",
        "    del model, result, transcribed_text, inputs, translated_ids\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    # Lip-Sync with Wav2Lip\n",
        "    subprocess.run([\n",
        "        \"python\", \"Wav2Lip/inference.py\",\n",
        "        \"--checkpoint_path\", \"checkpoints/wav2lip_gan.pth\",\n",
        "        \"--face\", video_path,\n",
        "        \"--audio\", tts_audio_path,\n",
        "        \"--wav2lip_batch_size\", \"1\"\n",
        "    ])\n",
        "\n",
        "    # Return the lip-synced video\n",
        "    synced_video_path = \"results/result_voice.mp4\"\n",
        "    return send_file(synced_video_path, as_attachment=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, port))\n",
        "    app.run(port=port)"
      ],
      "metadata": {
        "id": "P4fAY1wigFgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST APP"
      ],
      "metadata": {
        "id": "fP0JhrJ9DQdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install flask pyngrok\n",
        "\n",
        "# Ensure Wav2Lip and its requirements are installed\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git\n",
        "!pip install -r Wav2Lip/requirements.txt\n",
        "!pip install librosa moviepy gdown\n",
        "!pip install transformers TTS\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "%cd Wav2Lip\n",
        "!gdown --id 1_OvqStxNxLc7bXzlaVG5sz695p-FVfYY -O checkpoints/wav2lip_gan.pth"
      ],
      "metadata": {
        "id": "hIrCZp7IDY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "token=\"2iljjpUSwsZy9UlIuGW0aIiDutE_2v5pTXDEfrPAcKHeCnF5a\"\n",
        "ngrok.set_auth_token(token)"
      ],
      "metadata": {
        "id": "pb9yd7KaFFWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "import whisper\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from TTS.api import TTS\n",
        "import moviepy.editor as mp\n",
        "%cd Wav2Lip\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Create the uploads directory if it doesn't exist\n",
        "if not os.path.exists('uploads'):\n",
        "    os.makedirs('uploads')\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "\n",
        "@app.route(\"/upload\", methods=[\"POST\"])\n",
        "def upload():\n",
        "    video_file = request.files['video']\n",
        "    ref_audio_file = request.files['ref_audio']\n",
        "\n",
        "    video_path = os.path.join(\"uploads\", video_file.filename)\n",
        "    ref_audio_path = os.path.join(\"uploads\", ref_audio_file.filename)\n",
        "\n",
        "    video_file.save(video_path)\n",
        "    ref_audio_file.save(ref_audio_path)\n",
        "\n",
        "    # Extract audio from video\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio_file = \"extracted_audio.wav\"\n",
        "    video.audio.write_audiofile(audio_file)\n",
        "\n",
        "    # Transcribe Audio with Whisper\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_file)\n",
        "    transcribed_text = result['text']\n",
        "    print(\"Transcribed Text:\", transcribed_text)\n",
        "\n",
        "    # Translate Text to Hindi using NLLB-200 model\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    translation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Define language codes\n",
        "    src_lang = \"eng_Latn\"\n",
        "    tgt_lang = \"hin_Deva\"\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"{src_lang} {transcribed_text} {tgt_lang}\"\n",
        "\n",
        "    # Tokenize and translate\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "    translated_ids = translation_model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Translated Text:\", translated_text)\n",
        "\n",
        "    # Convert Text to Speech in Hindi\n",
        "    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
        "    tts_audio_path = \"translated_audio.mp3\"\n",
        "    tts.tts_to_file(translated_text,\n",
        "                    file_path=tts_audio_path,\n",
        "                    speaker_wav=ref_audio_path, # Use uploaded reference audio file\n",
        "                    language=\"hi\")\n",
        "\n",
        "    del model, result, transcribed_text, inputs, translated_ids\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    # # Lip-Sync with Wav2Lip\n",
        "    # subprocess.run([\n",
        "    #     \"python\", \"Wav2Lip/inference.py\",\n",
        "    #     \"--checkpoint_path\", \"checkpoints/wav2lip_gan.pth\",\n",
        "    #     \"--face\", video_path,\n",
        "    #     \"--audio\", tts_audio_path,\n",
        "    #     \"--wav2lip_batch_size\", \"1\"\n",
        "    # ])\n",
        "\n",
        "    # # Return the lip-synced video\n",
        "    # %cd Wav2Lip\n",
        "    # synced_video_path = \"results/result_voice.mp4\"\n",
        "    # %cd ..\n",
        "    # return send_file(synced_video_path, as_attachment=True)\n",
        "\n",
        "    result_video_path = \"results/result_voice.mp4\"\n",
        "    subprocess.run([\n",
        "        \"python\", \"inference.py\",\n",
        "        \"--checkpoint_path\", \"checkpoints/wav2lip_gan.pth\",\n",
        "        \"--face\", video_path,\n",
        "        \"--audio\", tts_audio_path,\n",
        "        \"--outfile\", result_video_path,\n",
        "        \"--wav2lip_batch_size\", \"1\"\n",
        "    ])\n",
        "\n",
        "    # Check if the output file is created\n",
        "    if not os.path.exists(result_video_path):\n",
        "        return jsonify({\"error\": \"Lip-sync failed, result video not created.\"}), 500\n",
        "\n",
        "    # Return the lip-synced video\n",
        "    return send_file(result_video_path, as_attachment=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, port))\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "id": "4hoPzn5bgKkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a503620-5b58-4291-ef00-98edaaec8991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Wav2Lip'\n",
            "/content/Wav2Lip\n",
            " * ngrok tunnel \"NgrokTunnel: \"https://458f-34-91-103-175.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000/\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in extracted_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed Text:  Hello, my name is Anuj and we have a demonstration with Nameshah in 5 minutes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "the `lang_code_to_id` attribute is deprecated. The logic is natively handled in the `tokenizer.adder_tokens_decoder` this attribute will be removed in `transformers` v4.38\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
            "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text: नमस्ते, मेरा नाम अनुज है और हम 5 मिनट में नामशाह के साथ एक प्रदर्शन है.\n",
            " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
            " > Using model: xtts\n",
            " > Text splitted to sentences.\n",
            "['नमस्ते, मेरा नाम अनुज है और हम 5 मिनट में नामशाह के साथ एक प्रदर्शन है.']\n",
            " > Processing time: 6.363751173019409\n",
            " > Real-time factor: 0.7497687086703748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [08/Jul/2024 07:51:43] \"POST /upload HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fin9TV6PL3F",
        "outputId": "83157843-f350-4361-c34f-9688321622ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio.py                \u001b[0m\u001b[01;34mface_detection\u001b[0m/      \u001b[01;34mmodels\u001b[0m/           \u001b[01;34mresults\u001b[0m/\n",
            "\u001b[01;34mcheckpoints\u001b[0m/            \u001b[01;34mfilelists\u001b[0m/           preprocess.py     \u001b[01;34mtemp\u001b[0m/\n",
            "color_syncnet_train.py  hparams.py           \u001b[01;34m__pycache__\u001b[0m/      translated_audio.mp3\n",
            "\u001b[01;34mevaluation\u001b[0m/             hq_wav2lip_train.py  README.md         \u001b[01;34muploads\u001b[0m/\n",
            "extracted_audio.wav     inference.py         requirements.txt  wav2lip_train.py\n"
          ]
        }
      ]
    }
  ]
}