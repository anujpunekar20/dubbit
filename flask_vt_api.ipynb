{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNP8hoe46Z49U2GJ2Qk6pOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujpunekar20/video-translator/blob/main/flask_vt_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install flask, ngrok and other libraries/modules\n"
      ],
      "metadata": {
        "id": "IRHr2RW7SDYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install flask pyngrok\n",
        "\n",
        "# Ensure Wav2Lip and its requirements are installed\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git\n",
        "!pip install -r Wav2Lip/requirements.txt\n",
        "!pip install librosa moviepy gdown\n",
        "!pip install transformers TTS\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "%cd Wav2Lip\n",
        "!gdown --id 1_OvqStxNxLc7bXzlaVG5sz695p-FVfYY -O checkpoints/wav2lip_gan.pth"
      ],
      "metadata": {
        "id": "hIrCZp7IDY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flask app"
      ],
      "metadata": {
        "id": "-Trqj0bWWt7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "token=\"your-ngrok-authtoken\"\n",
        "ngrok.set_auth_token(token)"
      ],
      "metadata": {
        "id": "pb9yd7KaFFWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "import whisper\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from TTS.api import TTS\n",
        "import moviepy.editor as mp\n",
        "%cd Wav2Lip\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Create the uploads directory if it doesn't exist\n",
        "if not os.path.exists('uploads'):\n",
        "    os.makedirs('uploads')\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "\n",
        "@app.route(\"/upload\", methods=[\"POST\"])\n",
        "def upload():\n",
        "    video_file = request.files['video']\n",
        "    ref_audio_file = request.files['ref_audio']\n",
        "\n",
        "    video_path = os.path.join(\"uploads\", video_file.filename)\n",
        "    ref_audio_path = os.path.join(\"uploads\", ref_audio_file.filename)\n",
        "\n",
        "    video_file.save(video_path)\n",
        "    ref_audio_file.save(ref_audio_path)\n",
        "\n",
        "    # Extract audio from video\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio_file = \"extracted_audio.wav\"\n",
        "    video.audio.write_audiofile(audio_file)\n",
        "\n",
        "    # Transcribe Audio with Whisper\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_file)\n",
        "    transcribed_text = result['text']\n",
        "    print(\"Transcribed Text:\", transcribed_text)\n",
        "\n",
        "    # Translate Text to Hindi using NLLB-200 model\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    translation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Define language codes\n",
        "    src_lang = \"eng_Latn\"\n",
        "    tgt_lang = \"hin_Deva\"\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"{src_lang} {transcribed_text} {tgt_lang}\"\n",
        "\n",
        "    # Tokenize and translate\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "    translated_ids = translation_model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Translated Text:\", translated_text)\n",
        "\n",
        "    # Convert Text to Speech in Hindi\n",
        "    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
        "    tts_audio_path = \"translated_audio.mp3\"\n",
        "    tts.tts_to_file(translated_text,\n",
        "                    file_path=tts_audio_path,\n",
        "                    speaker_wav=ref_audio_path, # Use uploaded reference audio file\n",
        "                    language=\"hi\")\n",
        "\n",
        "    del model, result, transcribed_text, inputs, translated_ids\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    result_video_path = \"results/result_voice.mp4\"\n",
        "    subprocess.run([\n",
        "        \"python\", \"inference.py\",\n",
        "        \"--checkpoint_path\", \"checkpoints/wav2lip_gan.pth\",\n",
        "        \"--face\", video_path,\n",
        "        \"--audio\", tts_audio_path,\n",
        "        \"--outfile\", result_video_path,\n",
        "        \"--wav2lip_batch_size\", \"1\"\n",
        "    ])\n",
        "\n",
        "    # Check if the output file is created\n",
        "    if not os.path.exists(result_video_path):\n",
        "        return jsonify({\"error\": \"Lip-sync failed, result video not created.\"}), 500\n",
        "\n",
        "    # Return the lip-synced video\n",
        "    return send_file(result_video_path, as_attachment=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, port))\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "id": "4hoPzn5bgKkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "4fin9TV6PL3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}